{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReS2 Slip Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16022873, 0.07804659])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.models as models\n",
    "from model import StructNet\n",
    "import numpy as np\n",
    "from math import cos, sin, tan,degrees, radians\n",
    "\n",
    "def shift_trans(x,y):\n",
    "    all_targets = np.array([[x,y,1.]])\n",
    "\n",
    "    all_targets_symmetric = all_targets.copy()\n",
    "    all_targets_symmetric[:, 1] = -all_targets_symmetric[:, 1]\n",
    "\n",
    "    all_targets_filp = all_targets_symmetric.copy()\n",
    "    all_targets_filp[:, :2] = (0, 0) - (all_targets_symmetric[:, :2] - (0, 0))\n",
    "    points = np.concatenate([all_targets_filp,all_targets_symmetric])\n",
    "\n",
    "    condition_indices = np.where(points[:, 0]<0)\n",
    "\n",
    "    shift_value = 6.42\n",
    "    points[condition_indices, 0] += shift_value \n",
    "    condition_indices = np.where(points[:, 1]<0)\n",
    "    points[condition_indices, 0] += -3.21\n",
    "    points[condition_indices, 1] += 5.71\n",
    "    condition_indices = np.where(1.74*points[:,0]+points[:, 1]<0)\n",
    "    points[condition_indices, 0] += 6.42\n",
    "    condition_indices2 = np.where(1.74*points[:,0]+points[:, 1]>11.21)\n",
    "    points[condition_indices2, 0] += -6.42\n",
    "\n",
    "    index = []\n",
    "    for i in range(points.shape[0]):\n",
    "        if (5.71/(3.27+6.42))*points[i][0] + points[i][1] > (5.71/(3.27+6.42))*6.42:\n",
    "            index.append(i)\n",
    "\n",
    "    points = np.delete(points,index,axis=0)\n",
    "\n",
    "    angle_in_degrees1 = 119.8- 90\n",
    "    angle_in_radians1 = radians(angle_in_degrees1)\n",
    "    angle_in_degrees2 = 60.2\n",
    "    angle_in_radians2 = radians(angle_in_degrees2)\n",
    "    new_x = [p[0]+p[1]*tan(angle_in_radians1) for p in points]\n",
    "    new_y = [p[1]/cos(angle_in_radians1) for p in points]\n",
    "    target_new = np.stack([new_x, new_y],axis=1)\n",
    "\n",
    "    return target_new[0]\n",
    "\n",
    "weight_path = '/vepfs/fs_ckps/ycjin/nudt/sim/ablation_update/model_params-800.pth'\n",
    "image_path = '/vepfs/fs_users/ycjin/3.png'\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = StructNet(output_dim = 3)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(weight_path))\n",
    "model.eval()\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = image.resize((1024,1024))\n",
    "# image = TF.hflip(image)\n",
    "image_list = []\n",
    "\n",
    "for i in range(16):\n",
    "    crop_size = 512 + i * 10\n",
    "    image_c = TF.crop(image, 0, 0, crop_size, crop_size)\n",
    "    image_c = image_c.resize([512,512])\n",
    "    data = TF.to_tensor(image_c)\n",
    "    image_list.append(data)\n",
    "data = torch.stack(image_list,dim = 0).to(device)\n",
    "output = model(data)\n",
    "output = output.mean(dim=0)[:2].cpu().detach()\n",
    "shift_trans(output[0], output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.30744218, 0.07674885])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_trans(-1.2693,  0.0666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7790,  0.1226],\n",
       "        [-2.8051,  0.0977],\n",
       "        [-2.7986,  0.0947],\n",
       "        [-2.8250,  0.0714],\n",
       "        [-1.6443,  0.0890],\n",
       "        [ 0.8163,  0.0654],\n",
       "        [-0.7207,  0.0639],\n",
       "        [-0.8000,  0.0326],\n",
       "        [ 0.8027,  0.0583],\n",
       "        [-1.2693,  0.0666],\n",
       "        [-2.2677,  0.0508],\n",
       "        [-1.7738,  0.0287],\n",
       "        [-2.7097,  0.0559],\n",
       "        [-0.8225,  0.0583],\n",
       "        [ 1.7957,  0.0450],\n",
       "        [ 1.8579,  0.0826]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)[:,:2].cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "k_model = KMeans(n_clusters=3)\n",
    "k_model.fit(model(data).cpu().detach())\n",
    "predict_y = k_model.predict(model(data).cpu().detach())\n",
    "predict_y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
